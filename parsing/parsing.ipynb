{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import pymongo\n",
    "import asyncio\n",
    "from dotenv import dotenv_values\n",
    "import logging\n",
    "import aiohttp\n",
    "import re\n",
    "from time import sleep\n",
    "from random import choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing approach\n",
    "To implement Wiki parser I have decided to use reqiests + BeautifulSoup libraries as wikipedia do not have dynamic page creation as well as some parsing protection (they just kindly ask not to send to many requests in short time persion).\n",
    "\n",
    "### Parser implementation\n",
    "I have implemented & used this code in .py file (because it was much more simpler to profile, run in automatic mode), here just a copy of it. \n",
    "\n",
    "The core parser idea is: \n",
    "1) Extract minimum information of films and put it with urls in db\n",
    "2) Triverse db, check whether film has all required fields (title, director, box office...)\n",
    "    and only if it has not we parse it by its url. \n",
    "\n",
    "This approach saved my time many times, as it is a normal case, when parser meets some \"strange\" unparsable page and just falls with error and all progress is lost. \n",
    "\n",
    "#### Methods: \n",
    "1) Class constructor\n",
    "Opens a config file, to get mongo-db login & password that stored in .env file: \n",
    "    ```\n",
    "    LOGIN=some_login\n",
    "    PASSWORD=some_password\n",
    "    ```\n",
    "\n",
    "    Creates an async connection to db (whole parser written in async mode to speedup parsing, in my tests it shows 2.5 performance)\n",
    "\n",
    "2) extract_film_urls method\n",
    "    - retrieves the page\n",
    "    - searches for the target table\n",
    "    - extracts only urls and box_office (it was simpler to parse it from here, whether from film page) fields     \n",
    "    - stores all of them in DB\n",
    "\n",
    "3) parse_films_data method\n",
    "    - checks whether db has unparsed films (title field missing)\n",
    "    - if there are films to parse: \n",
    "    -  parses data of each missing film using Parser.parse_film_data\n",
    "    - puts data in db\n",
    "\n",
    "4) parse_film_data method <br>\n",
    "    Core method of the parser: \n",
    "    - wait some random time (if we do not it wikipedia can ignore our page request) 0-0.7 seconds is enough. \n",
    "    - retrieve whole page\n",
    "    - get table (each film page has special table with all data needed for assignment)\n",
    "    - extract all fields (except image) from that table in dictionary   \n",
    "    - **Title** - can be extracted easily, nothing to say. \n",
    "    - **Release date** - Just asked llm to write me a regex pattern to match (as date-format differs from page to page)\n",
    "    - **Country** - was the hardest field to parse, has many variations from page to page. (even field name may differ from page to page). To properly handle it, I've just wrote simplest code to parse all what I can, collected urls of pages that were parsed bad (raised an error on them and collected), observed what I can do.... and did it until there is no unparsable pages. \n",
    "    - **Director** - there also were problems with this fields, I have hardcoded extraction of all strings from following field and just observed that \"Animation director:\" and some similar fields always contained \":\", so I dropped everything that had \":\".  \n",
    "\n",
    "\n",
    "5) fetch_page method <br>\n",
    "    As requests do not support async requests, I've found such async implementation. It is equivalent requests.get, but can work in async mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self):\n",
    "        logging.basicConfig(filename='logs/parser.log', level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.info('Parser started.') \n",
    "        self.config = dotenv_values(\".env\")        \n",
    "        \n",
    "        self.listing_url = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\" \n",
    "        self.mongo_connection = pymongo.AsyncMongoClient(f\"mongodb://{self.config['LOGIN']}:{self.config['PASSWORD']}@127.0.0.1\")\n",
    "    \n",
    "    async def extract_film_urls(self, url: str):  \n",
    "        self.logger.info('JOB: extract_film_urls') \n",
    "        \n",
    "        db = self.mongo_connection['films']\n",
    "        collection = db.films\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        \n",
    "        if request.status_code != 200: \n",
    "            self.logger.error('Could not get page.')   \n",
    "            return\n",
    "        \n",
    "        content = BeautifulSoup(request.content, 'html.parser')\n",
    "        tables = content.find_all('table', class_='wikitable plainrowheaders')\n",
    "        table = tables[max(enumerate(tables), key=lambda x: len(x[1].find_all('tr')))[0]]\n",
    "\n",
    "        film_urls = [] \n",
    "        for film_data_raw in table.find_all('tr')[1:]:\n",
    "            url = 'https://en.wikipedia.org' + film_data_raw.find('i').find('a').get('href')\n",
    "            \n",
    "            box_office = film_data_raw.find_all('td')[1].text.replace(',', '') \n",
    "            box_office = ''.join(char if char.isnumeric() else ' ' for char in box_office[1:]).split()[0]\n",
    "\n",
    "            contains = await collection.find_one({\"url\": url})\n",
    "            if contains is None:\n",
    "                film_urls.append({'url': url, 'box_office': box_office})\n",
    "                \n",
    "        self.logger.info(f'Found {len(film_urls)} new films.')   \n",
    "        if len(film_urls) == 0: \n",
    "            return\n",
    "        \n",
    "        self.logger.info('Inserting new films to database...') \n",
    "        await collection.insert_many(film_urls)\n",
    "        self.logger.info('Successfully inserted.') \n",
    "        \n",
    "    async def parse_films_data(self): \n",
    "        self.logger.info('JOB: parse_films_data') \n",
    "        \n",
    "        db = self.mongo_connection['films']\n",
    "        collection = db.films\n",
    "        \n",
    "        films = await collection.find({\"url\": {\"$exists\": True}, \"title\": {\"$exists\": False}}).to_list(length=None)\n",
    "        \n",
    "        film_promises = []\n",
    "        for film in films:\n",
    "            film_promises.append(self.parse_film_data(film['url']))\n",
    "\n",
    "        write_promises = []\n",
    "        for film, data in zip(films, await asyncio.gather(*film_promises)):\n",
    "            write_promises.append(collection.update_one({\"_id\": film['_id']}, {\"$set\": data}))\n",
    "            \n",
    "        await asyncio.gather(*write_promises)\n",
    "            \n",
    "    async def parse_film_data(self, url: str):\n",
    "        self.logger.info(f'JOB:      parse_film_data {url}') \n",
    "        sleep(choice([0, 0.15, 0.4, 0.5, 0.7]))\n",
    "        \n",
    "        page_content = await self.fetch_page(url)\n",
    "        content = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "        # title, release_year, director, country\n",
    "        table = content.find('table', class_='infobox vevent').find('tbody')\n",
    "        rows = table.find_all('tr')\n",
    "\n",
    "        fields = {row.find('th').text: row.find('td') for row in rows[1:] if row.find('th') is not None}\n",
    "                \n",
    "        film = dict()\n",
    "        # title processing \n",
    "        film['title'] = rows[0].find('th').text \n",
    "        \n",
    "        # release_year processing\n",
    "        pattern = r'\\b(1[7-9]\\d{2}|2[0-1]\\d{2})\\b'\n",
    "        \n",
    "        field = (fields.get('Release dates') or fields.get('Release date')).text\n",
    "        film['release_year'] = int(re.findall(pattern, field)[0])\n",
    "        \n",
    "        # director processing\n",
    "        if fields['Directed by'].find('li') is not None:\n",
    "            film['director'] = [(name.text[:name.text.find(' (')]  if '(' in name.text else name.text)\n",
    "                                for name in fields['Directed by'].find_all('li') if ':' not in name.text]\n",
    "        else: \n",
    "            film['director'] = [fields.get('Directed by').find('a').text]\n",
    "        \n",
    "        # country processing\n",
    "        if 'Country' in fields:\n",
    "            film['country'] = fields.get('Country').text    \n",
    "        else:\n",
    "            if fields.get('Countries').find('li') is not None:\n",
    "                film['country'] = fields.get('Countries').find('li').text\n",
    "            else:\n",
    "                field = str(fields.get('Countries')) \n",
    "                film['country'] = field[field.find('>') + 1:field[1:].find('<') + 1] \n",
    "                \n",
    "        if '[' in film['country']:\n",
    "            film['country'] = film['country'][:film['country'].find('[')]\n",
    "            \n",
    "        self.logger.info(f'JOB DONE: parse_film_data {url}') \n",
    "        return film\n",
    "        \n",
    "    async def fetch_page(self, url):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url) as response:\n",
    "                return await response.text()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
